{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow of Project\n",
    "This project utilizes the Data Science Project Life Cycle, which has the following steps:\n",
    "\n",
    "1. Business Understanding\n",
    "2. Data Collection\n",
    "3. Data Preparation\n",
    "4. Exploratory Data Analysis\n",
    "5. Modelling\n",
    "6. Model Evaluation\n",
    "7. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to concepts of Economic Growth and Productive Employment\n",
    "\n",
    "* Gross Domestic Product(GDP) as a measure of Economic Growth\n",
    "> * GDP = value of goods and services produced by the nations economy - value of goods and services used up in production\n",
    "> * Two measures of GDP will be used in this study, the Contribution_by_GDP and Growth_by_GDP\n",
    "\n",
    "* Productive Employment\n",
    "> * Productive Employment is employment yielding sufficient returns to labour, to permit a worker and his/her dependents a level of consumption above the poverty line\n",
    "> * The International Labour Organization (ILO) has set the International Poverty Line to USD 2 (USD 1.90) a day; hence any person earning below USD 2 a day is considered poor, a group the ILO refers to as the 'Working Poor'\n",
    "> * The working poor in this case will be the people earning below KSh 10,000 a month (Wage_bracket_0_to_9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------- End of Business Understanding Section (Part 1)----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Collection\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Perform Data Extraction from PDF Files using camelot-py Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "\n",
    "* Installing dependecies for camelot-py, which include GhostScript and TKinter, on local machine\n",
    "* Installing camelot-py module\n",
    "* Testing camelot-py module\n",
    "* Downloading yearly Statistical Abstract files from Kenya National Bureau of Statitics (KNBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources of data (PDF files, courtesy of KNBS)\n",
    "\n",
    "\n",
    "* Statistical Abstract 2013\n",
    "* Statistical Abstract 2014\n",
    "* Statistical Abstract 2015\n",
    "* Statistical Abstract 2017\n",
    "* Statistical Abstract 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract data from the pdf files, I connect to local runtime, which has camelot-py module installed; to use the module for extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "1. Set up notebook server to allow connection to local runtime; using the command provided in the next section\n",
    "2. Look up for desired tables in pdf files, noting page numbers\n",
    "3. Extract tables from the pdf files using the page numbers\n",
    "4. Export tables to csv files, which will then be used in the data preparation section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command for starting notebook server\n",
    "\n",
    "jupyter notebook in command prompt or in terminal\n",
    "\n",
    "import camelot module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting tables from the given datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Abstract 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables: 1\n",
      "PARSING REPORT\n",
      "{'accuracy': 95.47, 'whitespace': 11.79, 'order': 1, 'page': 285}\n"
     ]
    }
   ],
   "source": [
    "# Statistical Abstract 2013\n",
    "pathPdf = 'D:/Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/PDFs/PDFs/STATISTICAL ABSTRACT 2013.pdf'\n",
    "pathDataset = \"D:/Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/CSV FILES/\"\n",
    "tables = camelot.read_pdf(pathPdf, pages='285', flavor='stream', strip_text='*+')\n",
    "\n",
    "numOfTables=tables.n\n",
    "print('Number of tables: ' + str(numOfTables))\n",
    "\n",
    "#Parsing Report\n",
    "print('PARSING REPORT')\n",
    "print(tables[0].parsing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving table from pdf to csv\n",
    "tables[0].to_csv(pathDataset + '/Wage Employment 2011.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Abstract 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables: 6\n",
      "PARSING REPORT\n",
      "{'accuracy': 99.36, 'whitespace': 8.12, 'order': 1, 'page': 74}\n",
      "{'accuracy': 99.63, 'whitespace': 10.68, 'order': 1, 'page': 77}\n",
      "{'accuracy': 91.57, 'whitespace': 31.06, 'order': 1, 'page': 265}\n",
      "{'accuracy': 97.76, 'whitespace': 14.48, 'order': 1, 'page': 266}\n",
      "{'accuracy': 99.17, 'whitespace': 14.48, 'order': 1, 'page': 267}\n",
      "{'accuracy': 99.65, 'whitespace': 18.93, 'order': 1, 'page': 268}\n"
     ]
    }
   ],
   "source": [
    "pathpdf = \"D:/Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/PDFs/PDFs/STATISTICAL ABSTRACT 2014.pdf\"\n",
    "tables = camelot.read_pdf(pathpdf,pages = \"74,77,265,266,267,268\", flavor=\"stream\", strip_text =\"*+\\n\" )\n",
    "\n",
    "nOfTables = tables.n\n",
    "print(\"Number of tables: \" + str(nOfTables))\n",
    "\n",
    "#parsing Report\n",
    "print(\"PARSING REPORT\")\n",
    "\n",
    "for i in range(nOfTables):\n",
    "    print(tables[i].parsing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting Tables to CSV Files\n",
    "tables[0].to_csv(pathDataset + '/Contribution to GDP by Percent 2009-2013.csv')\n",
    "tables[1].to_csv(pathDataset + '/Growth  of GDP by Activity 2009-2013.csv')\n",
    "tables[2].to_csv(pathDataset + '/Wage Employment 2010-2013.csv')\n",
    "tables[3].to_csv(pathDataset + '/Wage Employment 2012.csv')\n",
    "tables[4].to_csv(pathDataset + '/Wage Employment 2013.csv')\n",
    "tables[5].to_csv(pathDataset + '/Wage Employment by Sex and Income 2010-2013.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Abstract 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables: 4\n",
      "PARSING REPORT\n",
      "{'accuracy': 92.98, 'whitespace': 24.84, 'order': 1, 'page': 101}\n",
      "{'accuracy': 99.79, 'whitespace': 25.0, 'order': 2, 'page': 101}\n",
      "{'accuracy': 92.98, 'whitespace': 24.84, 'order': 1, 'page': 102}\n",
      "{'accuracy': 99.79, 'whitespace': 25.0, 'order': 2, 'page': 102}\n"
     ]
    }
   ],
   "source": [
    "# Statistical Abstract 2017\n",
    "pathPdf = 'D:/Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/PDFs/PDFs/STATISTICAL ABSTRACT 2017.pdf'\n",
    "tables = camelot.read_pdf(pathPdf, pages='101,102', flavor='stream', strip_text='*+\\n')\n",
    "\n",
    "numOfTables=tables.n\n",
    "print('Number of tables: ' + str(numOfTables))\n",
    "\n",
    "#Parsing Report\n",
    "print('PARSING REPORT')\n",
    "i=0\n",
    "while i < numOfTables:\n",
    "    print(tables[i].parsing_report)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[1].to_csv(pathDataset + '/Wage Employment 2015.csv')\n",
    "tables[3].to_csv(pathDataset + '/Wage Employment 2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Abstract 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables: 6\n",
      "PARSING REPORT\n",
      "{'accuracy': 99.55, 'whitespace': 9.21, 'order': 1, 'page': 30}\n",
      "{'accuracy': 99.62, 'whitespace': 8.97, 'order': 1, 'page': 32}\n",
      "{'accuracy': 88.77, 'whitespace': 29.29, 'order': 1, 'page': 63}\n",
      "{'accuracy': 99.94, 'whitespace': 22.76, 'order': 1, 'page': 64}\n",
      "{'accuracy': 99.95, 'whitespace': 22.67, 'order': 1, 'page': 65}\n",
      "{'accuracy': 99.44, 'whitespace': 19.71, 'order': 1, 'page': 66}\n"
     ]
    }
   ],
   "source": [
    "# Statistical Abstract 2019\n",
    "pathPdf = 'D:/Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/PDFs/PDFs/Statistical Abstract 2019.pdf'\n",
    "tables = camelot.read_pdf(pathPdf, pages='30,32,63,64,65,66', flavor='stream', strip_text='*+\\n')\n",
    "\n",
    "numOfTables=tables.n\n",
    "print('Number of tables: ' + str(numOfTables))\n",
    "\n",
    "#Parsing Report\n",
    "print('PARSING REPORT')\n",
    "i=0\n",
    "for i in range(numOfTables):\n",
    "    print(tables[i].parsing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting Tables to CSV Files\n",
    "tables[0].to_csv(pathDataset + '/Contribution to GDP by Percent 2012-2018.csv')\n",
    "tables[1].to_csv(pathDataset + '/Growth  of GDP by Activity 2012-2018.csv')\n",
    "tables[2].to_csv(pathDataset + '/Wage Employment 2014-2018.csv')\n",
    "tables[3].to_csv(pathDataset + '/Wage Employment 2017.csv')\n",
    "tables[4].to_csv(pathDataset + '/Wage Employment 2018.csv')\n",
    "tables[5].to_csv(pathDataset + '/Wage Employment by Sex and Income 2014-2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing data collection, we can now disconnect local runtime and switch to hosted runtime for the next sections "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------- End of Data Collection Section (Part 2)--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data Preparation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "1. Preparing datasets using Microsoft Excel\n",
    "2. Connection to hosted runtime\n",
    "3. Migrating prepared datasets from local disk to Google Drive\n",
    "4. Mounting Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "1.   Joining yearly datasets to a single dataset spanning all the years (2011 -2018)\n",
    "2.   Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access denied with the following error:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " \tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/uc?id=your_file_id \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# Replace 'your_file_id' with the actual file ID from your Google Drive shareable link\n",
    "file_id = 'your_file_id'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# Replace 'your_destination_path' with the path where you want to save the file\n",
    "output = 'your_destination_path'\n",
    "\n",
    "gdown.download(url, output, quiet=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import sys \n",
    "import numpy as np # linear algebra\n",
    "from scipy.stats import randint\n",
    "import pandas as pd # data processing, CSV file I/O \n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph \n",
    "import seaborn as sns # used for plot interactive graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Joining yearly datasets to a single dataset spanning all the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2011 = pd.read_csv(r\"D:\\Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/CSV FILES/PROJECT 1 FILES/Wage_Employment_and_GDP_2011.csv\")\n",
    "df2012 = pd.read_csv(r\"D:\\Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/CSV FILES/PROJECT 1 FILES/Wage_Employment_and_GDP_2012.csv\")\n",
    "df2013 = pd.read_csv(r\"D:\\Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/CSV FILES/PROJECT 1 FILES/Wage_Employment_and_GDP_2013.csv\")\n",
    "df2014 = pd.read_csv(r\"D:\\Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/CSV FILES/PROJECT 1 FILES/Wage_Employment_and_GDP_2014.csv\")\n",
    "df2015 = pd.read_csv(r\"D:\\Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/CSV FILES/PROJECT 1 FILES/Wage_Employment_and_GDP_2015.csv\")\n",
    "df2016 = pd.read_csv(r\"D:\\Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/CSV FILES/PROJECT 1 FILES/Wage_Employment_and_GDP_2016.csv\")\n",
    "df2017 = pd.read_csv(r\"D:\\Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/CSV FILES/PROJECT 1 FILES/Wage_Employment_and_GDP_2017.csv\")\n",
    "df2018 = pd.read_csv(r\"D:\\Hackathon/Hackathon (16) - Python Project/Hackathon (16) - Python Project/CSV FILES/PROJECT 1 FILES/Wage_Employment_and_GDP_2018.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking shape of Yearly dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 13)\n",
      "(21, 13)\n",
      "(21, 13)\n",
      "(21, 13)\n",
      "(21, 13)\n",
      "(21, 13)\n",
      "(21, 13)\n",
      "(21, 13)\n"
     ]
    }
   ],
   "source": [
    "list_df = [df2011,df2012,df2013,df2014,df2015,df2016,df2017,df2018]\n",
    "\n",
    "for i in list_df:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df2011, df2012, df2013, df2014, df2015, df2016, df2017, df2018]\n",
    "df = pd.concat(df_list,ignore_index=True,sort=False)\n",
    "df.to_csv(pathDataset + 'Wage_Employment_and_GDP_2011_to_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
